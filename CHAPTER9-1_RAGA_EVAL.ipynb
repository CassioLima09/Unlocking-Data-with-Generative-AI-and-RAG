{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a08b6520-84ec-4a6a-9778-84b359282682",
      "metadata": {
        "id": "a08b6520-84ec-4a6a-9778-84b359282682"
      },
      "source": [
        "# Evaluating semantic search vs hybrid search for retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "815b7181-56f8-4cf1-a370-1459bd8d50d9",
      "metadata": {
        "id": "815b7181-56f8-4cf1-a370-1459bd8d50d9"
      },
      "source": [
        "Using Ragas for evaluation: https://docs.ragas.io/en/latest/getstarted/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e0ad45-f5b2-4e57-bdc6-2a808a315d24",
      "metadata": {
        "scrolled": true,
        "id": "16e0ad45-f5b2-4e57-bdc6-2a808a315d24",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%pip install langchain_community\n",
        "%pip install langchain_experimental\n",
        "%pip install langchain-openai\n",
        "%pip install langchainhub\n",
        "%pip install chromadb\n",
        "%pip install langchain\n",
        "%pip install python-dotenv\n",
        "%pip install gradio\n",
        "%pip uninstall uvloop -y\n",
        "%pip install PyPDF2 -q --user\n",
        "%pip install rank_bm25\n",
        "%pip install ragas\n",
        "%pip install tqdm -q --user\n",
        "%pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f884314f-870c-4bfb-b6c1-a5b4801ec172",
      "metadata": {
        "id": "f884314f-870c-4bfb-b6c1-a5b4801ec172",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715740689537,
          "user_tz": 240,
          "elapsed": 10880,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "import chromadb\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())\n",
        "nest_asyncio.apply()\n",
        "import gradio as gr\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "## NEW\n",
        "import tqdm as notebook_tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import Dataset\n",
        "from ragas import evaluate\n",
        "from ragas.testset.generator import TestsetGenerator\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "from ragas.metrics import (\n",
        "    answer_relevancy,\n",
        "    faithfulness,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    answer_correctness,\n",
        "    answer_similarity\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you cannot use .env, save the file as env and use this code to access:\n",
        "_ = load_dotenv(dotenv_path='env.txt')"
      ],
      "metadata": {
        "id": "wKaTaVA0Ixtg",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715740818417,
          "user_tz": 240,
          "elapsed": 172,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "wKaTaVA0Ixtg",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "721241b4-32ab-476a-a5ac-9feab48459e5",
      "metadata": {
        "id": "721241b4-32ab-476a-a5ac-9feab48459e5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715740819346,
          "user_tz": 240,
          "elapsed": 126,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "# OpenAI Setup\n",
        "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LLMs/Embeddings\n",
        "# models:\n",
        "embedding_ada = \"text-embedding-ada-002\"\n",
        "model_gpt35=\"gpt-3.5-turbo-0125\"\n",
        "model_gpt4=\"gpt-4-turbo-2024-04-09\"\n",
        "\n",
        "embedding_function = OpenAIEmbeddings(model=embedding_ada, openai_api_key=openai.api_key)\n",
        "llm = ChatOpenAI(model=model_gpt35, openai_api_key=openai.api_key, temperature=0.0)\n",
        "generator_llm = ChatOpenAI(model=model_gpt35, openai_api_key=openai.api_key, temperature=0.0)\n",
        "critic_llm = ChatOpenAI(model=model_gpt4, openai_api_key=openai.api_key, temperature=0.0)"
      ],
      "metadata": {
        "id": "Tz7M7KSBl1y3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715740822344,
          "user_tz": 240,
          "elapsed": 528,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "Tz7M7KSBl1y3",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ad428a-3eb6-40ec-a1a5-62565ead1e5b",
      "metadata": {
        "id": "d3ad428a-3eb6-40ec-a1a5-62565ead1e5b"
      },
      "outputs": [],
      "source": [
        "#### INDEXING ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "98ccda2c-0f4c-41c5-804d-2227cdf35aa7",
      "metadata": {
        "id": "98ccda2c-0f4c-41c5-804d-2227cdf35aa7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715740836290,
          "user_tz": 240,
          "elapsed": 11835,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "# # Load the PDF and extract text\n",
        "pdf_path = \"google-2023-environmental-report.pdf\"\n",
        "pdf_reader = PdfReader(pdf_path)\n",
        "\n",
        "text = \"\"\n",
        "for page in pdf_reader.pages:\n",
        "    text += page.extract_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "927a4c65-aa05-486c-8295-2f99673e7c20",
      "metadata": {
        "id": "927a4c65-aa05-486c-8295-2f99673e7c20",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715740836294,
          "user_tz": 240,
          "elapsed": 17,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "# Split\n",
        "character_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "splits = character_splitter.split_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6b13568c-d633-464d-8c43-0d55f34cc8c1",
      "metadata": {
        "id": "6b13568c-d633-464d-8c43-0d55f34cc8c1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715740844787,
          "user_tz": 240,
          "elapsed": 8506,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "chroma_client = chromadb.Client()\n",
        "embedding_function = OpenAIEmbeddings()\n",
        "collection_name = \"google_environmental_report\"\n",
        "dense_documents = [Document(page_content=text, metadata={\"id\": str(i), \"source\": \"dense\"}) for i, text in enumerate(splits)]\n",
        "sparse_documents = [Document(page_content=text, metadata={\"id\": str(i), \"source\": \"sparse\"}) for i, text in enumerate(splits)]\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=dense_documents,\n",
        "    embedding=embedding_function,\n",
        "    collection_name=collection_name,\n",
        "    client=chroma_client\n",
        ")\n",
        "\n",
        "# Create dense retriever\n",
        "dense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
        "\n",
        "# Create sparse retriever\n",
        "sparse_retriever = BM25Retriever.from_documents(sparse_documents, k=10)\n",
        "\n",
        "# initialize the ensemble retriever\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[dense_retriever, sparse_retriever], weights=[0.5, 0.5], c=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ce8df01-925b-45b5-8fb8-17b5c40c581f",
      "metadata": {
        "id": "6ce8df01-925b-45b5-8fb8-17b5c40c581f"
      },
      "outputs": [],
      "source": [
        "#### RETRIEVAL and GENERATION ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "eb47c817-b5ac-4d90-84ee-4cd209e52a80",
      "metadata": {
        "id": "eb47c817-b5ac-4d90-84ee-4cd209e52a80",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715740844917,
          "user_tz": 240,
          "elapsed": 132,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "# Prompt\n",
        "prompt = hub.pull(\"jclemens24/rag-prompt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Relevance check prompt\n",
        "relevance_prompt_template = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Given the following question and retrieved context, determine if the context is relevant to the question.\n",
        "    Provide a score from 1 to 5, where 1 is not at all relevant and 5 is highly relevant.\n",
        "    Return ONLY the numeric score, without any additional text or explanation.\n",
        "\n",
        "    Question: {question}\n",
        "    Retrieved Context: {retrieved_context}\n",
        "\n",
        "    Relevance Score:\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "-gM_jXU-Y4-B",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715740845069,
          "user_tz": 240,
          "elapsed": 153,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "-gM_jXU-Y4-B",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e8975479-b3e3-481d-ad7b-08b4eb3faaef",
      "metadata": {
        "id": "e8975479-b3e3-481d-ad7b-08b4eb3faaef",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715740845069,
          "user_tz": 240,
          "elapsed": 1,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fd9db713-f705-4b65-800e-2c4e3d0e4ef4",
      "metadata": {
        "id": "fd9db713-f705-4b65-800e-2c4e3d0e4ef4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715740845069,
          "user_tz": 240,
          "elapsed": 1,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "def extract_score(llm_output):\n",
        "    try:\n",
        "        score = float(llm_output.strip())\n",
        "        return score\n",
        "    except ValueError:\n",
        "        return 0\n",
        "\n",
        "# Chain it all together with LangChain\n",
        "def conditional_answer(x):\n",
        "    relevance_score = extract_score(x['relevance_score'])\n",
        "    if relevance_score < 4:\n",
        "        return \"I don't know.\"\n",
        "    else:\n",
        "        return x['answer']\n",
        "\n",
        "rag_chain_from_docs = (\n",
        "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
        "    | RunnableParallel(\n",
        "        {\"relevance_score\": (\n",
        "            RunnablePassthrough()\n",
        "            | (lambda x: relevance_prompt_template.format(question=x['question'], retrieved_context=x['context']))\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        ), \"answer\": (\n",
        "            RunnablePassthrough()\n",
        "            | prompt\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )}\n",
        "    )\n",
        "    | RunnablePassthrough().assign(final_answer=conditional_answer)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain_similarity = RunnableParallel(\n",
        "    {\"context\": dense_retriever,\n",
        "     \"question\": RunnablePassthrough()\n",
        "}).assign(answer=rag_chain_from_docs)"
      ],
      "metadata": {
        "id": "qUJs89R6ZNip",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715740846993,
          "user_tz": 240,
          "elapsed": 156,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "qUJs89R6ZNip",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain_hybrid = RunnableParallel(\n",
        "    {\"context\": ensemble_retriever,\n",
        "     \"question\": RunnablePassthrough()\n",
        "}).assign(answer=rag_chain_from_docs)"
      ],
      "metadata": {
        "id": "_S6btCZhY-UV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715740848001,
          "user_tz": 240,
          "elapsed": 107,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "_S6btCZhY-UV",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question - Submitted to the similarity / dense vector search\n",
        "user_query = \"What are Google's environmental initiatives?\"\n",
        "result = rag_chain_similarity.invoke(user_query)\n",
        "retrieved_docs = result['context']\n",
        "\n",
        "print(f\"Original Question to Similarity Search: {user_query}\\n\")\n",
        "print(f\"Relevance Score: {result['answer']['relevance_score']}\\n\")\n",
        "print(f\"Final Answer:\\n{result['answer']['final_answer']}\\n\\n\")\n",
        "print(\"Retrieved Documents:\")\n",
        "for i, doc in enumerate(retrieved_docs, start=1):\n",
        "    print(f\"Document {i}: Document ID: {doc.metadata['id']} source: {doc.metadata['source']}\")\n",
        "    print(f\"Content:\\n{doc.page_content}\\n\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lTzNd07mZVTA"
      },
      "id": "lTzNd07mZVTA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b30177a-f9ab-45e4-812d-33b0f97325bd",
      "metadata": {
        "collapsed": true,
        "id": "8b30177a-f9ab-45e4-812d-33b0f97325bd"
      },
      "outputs": [],
      "source": [
        "# Question - Submitted to the hibrid / dense+sparse vector search\n",
        "user_query = \"What are Google's environmental initiatives?\"\n",
        "result = rag_chain_hybrid.invoke(user_query)\n",
        "retrieved_docs = result['context']\n",
        "\n",
        "print(f\"Original Question to Dense Search:: {user_query}\\n\")\n",
        "print(f\"Relevance Score: {result['answer']['relevance_score']}\\n\")\n",
        "print(f\"Final Answer:\\n{result['answer']['final_answer']}\\n\\n\")\n",
        "print(\"Retrieved Documents:\")\n",
        "for i, doc in enumerate(retrieved_docs, start=1):\n",
        "    print(f\"Document {i}: Document ID: {doc.metadata['id']} source: {doc.metadata['source']}\")\n",
        "    print(f\"Content:\\n{doc.page_content}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SIMILARITY SEARCH ONLY\n",
        "Google's environmental initiatives include empowering individuals to take action, working together with partners and customers, operating sustainably, achieving net-zero carbon emissions, water stewardship, and promoting a circular economy. They have implemented sustainability features in products like Google Maps, Google Nest thermostats, and Google Flights to help individuals make more sustainable choices. Google also supports various environmental organizations and initiatives, such as the iMasons Climate Accord, ReFED, and The Nature Conservancy, to accelerate climate action and address environmental challenges. Additionally, Google is involved in public policy advocacy and is committed to reducing its environmental impact through its operations and value chain.\n"
      ],
      "metadata": {
        "id": "vt0WA5gbZrZm"
      },
      "id": "vt0WA5gbZrZm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HYBRID SEARCH\n",
        "\n",
        "Google's environmental initiatives include empowering individuals to take action, working together with partners and customers, operating sustainably, achieving net-zero carbon emissions, focusing on water stewardship, promoting a circular economy, engaging with suppliers to reduce energy consumption and greenhouse gas emissions, and reporting environmental data. They also support public policy and advocacy for low-carbon economies, participate in initiatives like the iMasons Climate Accord and ReFED, and support projects with organizations like The Nature Conservancy. Additionally, Google is involved in initiatives with the World Business Council for Sustainable Development and the World Resources Institute to improve well-being for people and the planet. They are also working on using technology and platforms to organize information about the planet and make it actionable to help partners and customers create a positive impact."
      ],
      "metadata": {
        "id": "p0vqTbfKZcNo"
      },
      "id": "p0vqTbfKZcNo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SYNTHETIC DATA GENERATION"
      ],
      "metadata": {
        "id": "X7Jsr-wYaMtQ"
      },
      "id": "X7Jsr-wYaMtQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# generator with openai models\n",
        "generator = TestsetGenerator.from_langchain(\n",
        "    generator_llm,\n",
        "    critic_llm,\n",
        "    embedding_function\n",
        ")"
      ],
      "metadata": {
        "id": "aDqo-x7g3gS5"
      },
      "id": "aDqo-x7g3gS5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of Document objects from the chunks\n",
        "documents = [Document(page_content=chunk) for chunk in splits]\n",
        "\n",
        "#### FOR FOLLOWING CODE: Uncomment and run once to generate source for test dataset! ####\n",
        "# generate testset -\n",
        "testset = generator.generate_with_langchain_docs(\n",
        "    documents,\n",
        "    test_size=10,\n",
        "    distributions={\n",
        "        simple: 0.5,\n",
        "        reasoning: 0.25,\n",
        "        multi_context: 0.25\n",
        "    }\n",
        ")\n",
        "\n",
        "# comparison dataframe\n",
        "testset_df = testset.to_pandas()\n",
        "\n",
        "# save dataframes to CSV files in the specified directory\n",
        "testset_df.to_csv(os.path.join('testset_data.csv'), index=False)\n",
        "\n",
        "print(\"testset DataFrame saved successfully in the local directory.\")"
      ],
      "metadata": {
        "id": "Sa8ZqrCnaP6w"
      },
      "id": "Sa8ZqrCnaP6w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testset_df = testset.to_pandas()\n",
        "len(testset_df)\n",
        "\n",
        "# save dataframes to CSV files in the specified directory\n",
        "testset_df.to_csv(os.path.join('testset_data.csv'), index=False)\n",
        "\n",
        "print(\"testset DataFrame saved successfully in the local directory.\")"
      ],
      "metadata": {
        "id": "GGHFs-I8C8TB"
      },
      "id": "GGHFs-I8C8TB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pull data from saved testset, rather than generating above\n",
        "### load dataframs from CSV file\n",
        "saved_testset_df = pd.read_csv(os.path.join('testset_data.csv'))\n",
        "print(\"testset DataFrame loaded successfully from local directory.\")\n",
        "saved_testset_df.head(5)"
      ],
      "metadata": {
        "id": "GlvSvmK_csVe"
      },
      "id": "GlvSvmK_csVe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PREPARE SIMILARITY SEARCH DATASET"
      ],
      "metadata": {
        "id": "DJM33u_1c4-D"
      },
      "id": "DJM33u_1c4-D"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the DataFrame to a dictionary\n",
        "saved_testing_data = saved_testset_df.astype(str).to_dict(orient='list')\n",
        "\n",
        "# Create the testing_dataset\n",
        "saved_testing_dataset = Dataset.from_dict(saved_testing_data)\n",
        "\n",
        "# Update the testing_dataset to include only these columns -\n",
        "# \"question\", \"ground_truth\", \"answer\", \"contexts\"\n",
        "saved_testing_dataset_sm = saved_testing_dataset.remove_columns([\"evolution_type\", \"episode_done\"])"
      ],
      "metadata": {
        "id": "7RcpHhh2c4QH"
      },
      "id": "7RcpHhh2c4QH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_testing_dataset_sm"
      ],
      "metadata": {
        "id": "pVBD6b1Zc7n6"
      },
      "id": "pVBD6b1Zc7n6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EVAL SETS FOR EACH CHAIN"
      ],
      "metadata": {
        "id": "3TNKvrXOerLq"
      },
      "id": "3TNKvrXOerLq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate answers using the RAG chain\n",
        "def generate_answer(question, ground_truth, rag_chain):\n",
        "    result = rag_chain.invoke(question)\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"answer\": result[\"answer\"][\"final_answer\"],\n",
        "        \"contexts\": [doc.page_content for doc in result[\"context\"]],\n",
        "        \"ground_truth\": ground_truth\n",
        "    }"
      ],
      "metadata": {
        "id": "6HrK-klXqn4v"
      },
      "id": "6HrK-klXqn4v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the \"question\", \"answer\", \"contexts\", and \"ground_truth\" to the testing_dataset\n",
        "testing_dataset_similarity = saved_testing_dataset_sm.map(lambda x: generate_answer(x[\"question\"], x[\"ground_truth\"], rag_chain_similarity), remove_columns=saved_testing_dataset_sm.column_names)"
      ],
      "metadata": {
        "id": "iXgC8QvRqqop"
      },
      "id": "iXgC8QvRqqop",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the \"question\", \"answer\", \"contexts\", and \"ground_truth\" to the testing_dataset\n",
        "testing_dataset_hybrid = saved_testing_dataset_sm.map(lambda x: generate_answer(x[\"question\"], x[\"ground_truth\"], rag_chain_hybrid), remove_columns=saved_testing_dataset_sm.column_names)"
      ],
      "metadata": {
        "id": "ttDBKAhhshdY"
      },
      "id": "ttDBKAhhshdY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EVAL SCORING"
      ],
      "metadata": {
        "id": "oGmEJp6ZddS2"
      },
      "id": "oGmEJp6ZddS2"
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity search score\n",
        "score_similarity = evaluate(\n",
        "    testing_dataset_similarity,\n",
        "    metrics=[\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "        answer_correctness,\n",
        "        answer_similarity\n",
        "    ]\n",
        ")\n",
        "similarity_df = score_similarity.to_pandas()\n",
        "similarity_df"
      ],
      "metadata": {
        "id": "DQV_SbQcc7ga"
      },
      "id": "DQV_SbQcc7ga",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity search score\n",
        "score_hybrid = evaluate(\n",
        "    testing_dataset_hybrid,\n",
        "    metrics=[\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "        answer_correctness,\n",
        "        answer_similarity\n",
        "    ]\n",
        ")\n",
        "hybrid_df = score_hybrid.to_pandas()\n",
        "hybrid_df"
      ],
      "metadata": {
        "id": "4V23UEX6c7bR"
      },
      "id": "4V23UEX6c7bR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis that consolidates everything into easier to read scores\n",
        "# key columns to compare\n",
        "key_columns = [\n",
        "    'faithfulness',\n",
        "    'answer_relevancy',\n",
        "    'context_precision',\n",
        "    'context_recall',\n",
        "    'answer_correctness',\n",
        "    'answer_similarity'\n",
        "]\n",
        "\n",
        "# mean scores for each key column in similarity_df\n",
        "similarity_means = similarity_df[key_columns].mean()\n",
        "\n",
        "# mean scores for each key column in hybrid_df\n",
        "hybrid_means = hybrid_df[key_columns].mean()\n",
        "\n",
        "# comparison dataframe\n",
        "comparison_df = pd.DataFrame({'Similarity Run': similarity_means, 'Hybrid Run': hybrid_means})\n",
        "\n",
        "# difference between the means\n",
        "comparison_df['Difference'] = comparison_df['Similarity Run'] - comparison_df['Hybrid Run']\n",
        "\n",
        "# save dataframes to CSV files in the specified directory\n",
        "similarity_df.to_csv(os.path.join('similarity_run_data.csv'), index=False)\n",
        "hybrid_df.to_csv(os.path.join('hybrid_run_data.csv'), index=False)\n",
        "comparison_df.to_csv(os.path.join('comparison_data.csv'), index=True)\n",
        "\n",
        "print(\"Dataframes saved successfully in the local directory.\")"
      ],
      "metadata": {
        "id": "LxHAZHpzvt5H"
      },
      "id": "LxHAZHpzvt5H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ANALYSIS"
      ],
      "metadata": {
        "id": "IVkNOiaQw89r"
      },
      "id": "IVkNOiaQw89r"
    },
    {
      "cell_type": "code",
      "source": [
        "### load dataframes from CSV files\n",
        "sem_df = pd.read_csv(os.path.join('similarity_run_data.csv'))\n",
        "rec_df = pd.read_csv(os.path.join('hybrid_run_data.csv'))\n",
        "comparison_df = pd.read_csv(os.path.join('comparison_data.csv'), index_col=0)\n",
        "\n",
        "print(\"Dataframes loaded successfully from the local directory.\")\n",
        "\n",
        "# Analysis that consolidates everything into easier to read scores\n",
        "# reorder rows and add headings\n",
        "print(\"Performance Comparison:\")\n",
        "print(\"\\n**Retrieval**:\")\n",
        "print(comparison_df.loc[['context_precision', 'context_recall']])\n",
        "print(\"\\n**Generation**:\")\n",
        "print(comparison_df.loc[['faithfulness', 'answer_relevancy']])\n",
        "print(\"\\n**End-to-end evaluation**:\")\n",
        "print(comparison_df.loc[['answer_correctness', 'answer_similarity']])"
      ],
      "metadata": {
        "id": "SqzqmWy3w_Ik"
      },
      "id": "SqzqmWy3w_Ik",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis that consolidates everything into easier to read scores\n",
        "\n",
        "# plotting\n",
        "# create subplots for each category with increased spacing\n",
        "fig, axes = plt.subplots(3, 1, figsize=(12, 18), sharex=False)\n",
        "bar_width = 0.35\n",
        "categories = ['Retrieval', 'Generation', 'End-to-end evaluation']\n",
        "metrics = [\n",
        "    ['context_precision', 'context_recall'],\n",
        "    ['faithfulness', 'answer_relevancy'],\n",
        "    ['answer_correctness', 'answer_similarity']\n",
        "]\n",
        "\n",
        "# iterate over each category and plot the corresponding metrics\n",
        "for i, (category, metric_list) in enumerate(zip(categories, metrics)):\n",
        "    ax = axes[i]\n",
        "    x = range(len(metric_list))\n",
        "\n",
        "    # plot bars for Similarity Run (hex color #D51900)\n",
        "    similarity_bars = ax.bar(x, comparison_df.loc[metric_list, 'Similarity Run'], width=bar_width, label='Similarity Run', color='#D51900')\n",
        "\n",
        "    # add values to Similarity Run bars\n",
        "    for bar in similarity_bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.1%}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    # plot bars for Hybrid Run (hex color #992111)\n",
        "    hybrid_bars = ax.bar([i + bar_width for i in x], comparison_df.loc[metric_list, 'Hybrid Run'], width=bar_width, label='Hybrid Run', color='#992111')\n",
        "\n",
        "    # add values to Recursive Run bars\n",
        "    for bar in hybrid_bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.1%}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    ax.set_title(category, fontsize=14, pad=20)\n",
        "    ax.set_xticks([i + bar_width / 2 for i in x])\n",
        "    ax.set_xticklabels(metric_list, rotation=45, ha='right', fontsize=12)\n",
        "\n",
        "    # move the legend to the bottom right corner\n",
        "    ax.legend(fontsize=12, loc='lower right', bbox_to_anchor=(1, 0))\n",
        "\n",
        "# add overall labels and title\n",
        "fig.text(0.04, 0.5, 'Scores', va='center', rotation='vertical', fontsize=14)\n",
        "fig.suptitle('Performance Comparison', fontsize=16)\n",
        "\n",
        "# adjust spacing between subplots and increase the top margin\n",
        "plt.tight_layout(rect=[0.05, 0.03, 1, 0.95])\n",
        "plt.subplots_adjust(hspace=0.6, top=0.92)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MFE4BY_rxN7W"
      },
      "id": "MFE4BY_rxN7W",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "name": "CHAPTER9-1_RAGA_EVAL.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}