{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d124d22-de73-436b-86cd-9b162b469be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_community\n",
    "%pip install langchain_experimental \n",
    "%pip install langchain-openai \n",
    "%pip install langchainhub \n",
    "%pip install chromadb \n",
    "%pip install langchain\n",
    "%pip install beautifulsoup4\n",
    "%pip install python-dotenv\n",
    "%pip install gradio\n",
    "%pip uninstall uvloop -y\n",
    "!pip install gensim --user\n",
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f884314f-870c-4bfb-b6c1-a5b4801ec172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "import openai\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import chromadb\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())\n",
    "nest_asyncio.apply()\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba3468a-d7c2-4a79-8df2-c335542950f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you cannot use .env, save the file as env and use this code to access:\n",
    "_ = load_dotenv(dotenv_path='env.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "721241b4-32ab-476a-a5ac-9feab48459e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Setup\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a025b63-125d-4e2b-b092-c863d7ffff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain embedding for user query\n",
    "question = \"What are the advantages of using RAG?\"\n",
    "question_embedding = OpenAIEmbeddings().embed_query(question)\n",
    "first_5_numbers = question_embedding[:5]\n",
    "print(f\"User question embedding (first 5 dimensions): {first_5_numbers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8331bd-b531-4bb3-82d6-0bd300d941fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the size of the user query embedding\n",
    "embedding_size = len(question_embedding)\n",
    "print(f\"Embedding size: {embedding_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad428a-3eb6-40ec-a1a5-62565ead1e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98ccda2c-0f4c-41c5-804d-2227cdf35aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://kbourne.github.io/chapter1.html\",), \n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "927a4c65-aa05-486c-8295-2f99673e7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fecbaaa-df9a-47cf-909f-e1f327374efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\tTF\t\tIDF\n",
      "----\t\t--\t\t---\n",
      "000         \t0.16\t\t2.95\n",
      "1024        \t0.04\t\t2.95\n",
      "123         \t0.02\t\t2.95\n",
      "13          \t0.04\t\t2.95\n",
      "15          \t0.01\t\t2.95\n",
      "16          \t0.07\t\t2.95\n",
      "192         \t0.06\t\t2.95\n",
      "1m          \t0.08\t\t2.95\n",
      "200         \t0.08\t\t2.95\n",
      "2024        \t0.01\t\t2.95\n"
     ]
    }
   ],
   "source": [
    "# USING TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Extract the text content from the splits\n",
    "tfidf_documents = [split.page_content for split in splits]\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(tfidf_documents)\n",
    "\n",
    "# Get the vocabulary, term frequencies, and corresponding IDF values\n",
    "vocab = tfidf_vectorizer.get_feature_names_out()\n",
    "tf_values = tfidf_matrix.toarray()\n",
    "idf_values = tfidf_vectorizer.idf_\n",
    "\n",
    "# Create a list of tuples containing word, TF, and IDF values\n",
    "word_stats = list(zip(vocab, tf_values.sum(axis=0), idf_values))\n",
    "\n",
    "# Sort the list by IDF values in descending order\n",
    "word_stats.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Print the grid of top 10 words, TF, and IDF values\n",
    "print(\"Word\\t\\tTF\\t\\tIDF\")\n",
    "print(\"----\\t\\t--\\t\\t---\")\n",
    "for word, tf, idf in word_stats[:10]:\n",
    "    print(f\"{word:<12}\\t{tf:.2f}\\t\\t{idf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c4edd43-5b4b-4dd1-832a-174b7efdfdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Document:\n",
      " Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer’s needs are? You do not have to imagine it, that is what RAG does! Even smaller companies are not able to access much of their internal data resources very effectively. Larger companies are swimming in petabytes of data that is not readily accessible or is not being fully utilized. Prior to RAG, most of the services you saw that connected customers or employees with the data resources of the company were really just scratching the surface of what is possible compared to if they could access ALL of the data in the company. With the advent of RAG and generative AI in general, corporations are on the precipice of something really, really big. Comparing RAG with Model Fine-Tuning#\n",
      "Established Large Language Models (LLM), what we call the foundation models, can learn in two ways:\n",
      " Fine-tuning - With fine-tuning, you are adjusting the weights and/or biases that define the model's intelligence based on new training data. This directly impacts the model, permanently changing how it will interact with new inputs. Input/Prompts - This is where you actually \"use\" the model, using the prompt/input to introduce new knowledge that the LLM can act upon. Why not use fine-tuning in all situations?\n"
     ]
    }
   ],
   "source": [
    "# TD-IDF scoring for user query\n",
    "# User query embedding\n",
    "tfidf_user_query = [\"What are the advantages of RAG?\"]\n",
    "new_tfidf_matrix = tfidf_vectorizer.transform(tfidf_user_query)\n",
    "\n",
    "# Calculate cosine similarity between the new content and the original documents\n",
    "tfidf_similarity_scores = cosine_similarity(new_tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Find the index of the document with the highest similarity score\n",
    "tfidf_top_doc_index = tfidf_similarity_scores.argmax()\n",
    "\n",
    "# Print the text of the top document\n",
    "print(\"TF-IDF Top Document:\\n\", tfidf_documents[tfidf_top_doc_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72de19a4-f767-434d-b0fa-7efd08c2d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING AND SAVING DOC2VEC MODEL\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Extract the text content from the splits\n",
    "doc2vec_documents = [split.page_content for split in splits]\n",
    "\n",
    "# Tokenize the documents\n",
    "doc2vec_tokenized_documents = [doc.lower().split() for doc in doc2vec_documents]\n",
    "\n",
    "# Create tagged documents for Doc2Vec\n",
    "doc2vec_tagged_documents = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(doc2vec_tokenized_documents)]\n",
    "\n",
    "# Train the Doc2Vec model\n",
    "# Use this version first.\n",
    "# doc2vec_model = Doc2Vec(doc2vec_tagged_documents, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# After running the previous version of model, comment the previous line out and uncomment this one. Try it with 1536D vectors.\n",
    "doc2vec_model = Doc2Vec(doc2vec_tagged_documents, vector_size=1536, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the trained model to a file\n",
    "doc2vec_model.save(\"doc2vec_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7f65564-8adb-4a23-991f-e46dfbf6436a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doc2Vec Top Document:\n",
      " Vectors, Vectors, Vectors! A vector is a mathematical representation of your data. They are often referred to as the embeddings when talking specifically about natural language processing and LLMs. Vectors are one of the most important concepts to understand and there are many different parts of a RAG pipeline that utilize vectors. I felt it was bigger than just a quick definition, so I go into much more depth in the much larger next section dedicated to vectors. And beyond that, we literally spend two chapters (6 & 7) going over vectors and how they are used to find similar content. Vectors#\n",
      "It could be argued that understanding vectors and all the ways they are used in RAG is the most important part of this entire book. As mentioned above, vectors are simply the mathematical representations of your external data, and they are often referred to as embeddings. These representations capture semantic information in a format that can be processed by algorithms, facilitating tasks such as similarity search, which is a crucial step in the RAG process. Vectors typically have a specific dimension based on how many numbers are represented by them. For example, this is a 4 dimensional vector: [0.123, 0.321, 0.312, 0.231]\n",
      "\n",
      "If you didn’t know we were talking about vectors and you saw this in Python code, you might recognize this as a list of 4 floating points, and you aren’t too far off. Typically though, when working with vectors in Python, you actually want to recognize them as a Numpy Array. Numpy Arrays are generally more machine learning friendly because they are optimized to be processed much faster and efficiently than python lists, and they are more broadly recognized as the defacto representation of embeddings across machine learning packages like SciPy, Pandas, Scikit-Learn, TensorFlow, Keras, Pytorch, and many others. Numpy also enables you to perform vectorized math directly on the Numpy Array, such as performing element-wise operations, without having to code in loops and other approaches you might have to use if using a different type of sequence. When working with vectors for vectorization, they are often hundreds, or thousands of dimensions, which refers to the number of floating points present in the vector. So a 1024 dimension vector literally has 1024 floating points in a Numpy Array. Higher dimensionality can capture more detailed semantic information, which is crucial for accurately matching query inputs with relevant documents or data in RAG applications. In chapter 7, we cover the key role vectors and vector databases play In RAG implementation. And then in chapter 8, we will dive more into the concept of similarity searches, which utilize vectors to conduct the search much faster and efficiently. These are key concepts that will help you gain a much deeper understanding into how to better implement a RAG pipeline. Implementing RAG in AI Applications#\n",
      "Retrieval Augmented Generation (RAG) is rapidly becoming a cornerstone of GenAI platforms in the corporate world. RAG combines the power of information retrieval of internal or “new” data with generative language models to enhance the quality and relevance of generated text. This technique can be particularly useful for companies across various industries to improve their products, services, and operational efficiencies. Some examples of how RAG can be used include:\n",
      " Customer Support and Chatbots - These can exist without RAG, but when integrated with RAG, it can connect those chatbots with past customer interactions, FAQs, support documents, and anything else that was specific to that customer. Automated Reporting - RAG can assist in creating initial drafts or summarizing existing articles, research papers, and other types of unstructured data into more digestible formats. Product Descriptions - For e-commerce companies, RAG can be used to help generate or enhance product descriptions by retrieving information from similar products or manufacturer specifications. Searchability and Utility of Internal Knowledge Bases - RAG can improve access to internal knowledge bases. This can be achieved through the generation of summaries of documents or by providing direct answers to queries based on the content of internal documents, emails, and other resources. Searchability and Utility of General Knowledge Bases - In areas like legal and compliance, where companies need to have an understanding of a massive and growing general knowledge base, RAG can be implemented to retrieve and summarize relevant laws, regulations, and compliance documents. Other areas where this is applicable include research and development, medical, academia, patents, and technical documents. Innovation Scouting - Similar to searching general knowledge bases, but with a focus on new innovation, companies can use RAG to scan and summarize information from quality sources to identify trends and potential areas for new innovations that are relevant to that company's specialization. Content Personalization - RAG can be used by media and content platforms to personalize content recommendations or create customized summaries by retrieving information based on a user's past interactions and preferences. Product Recommendations - RAG can be used by e-commerce sites to enhance product recommendation engines, generate personalized descriptions, or highlight features based on the browsing and purchasing history of customers. Training and Education - RAG can be used by education organizations and corporate training programs to generate or customize learning materials based on specific needs and knowledge levels of the learners. With RAG, a much deeper level of internal knowledge from the organization can be incorporated into the educational curriculum in very customized ways to the individual or role. This book will help you understand how you can implement all of these game-changing initiatives in your company. Comparing RAG with Conventional Generative AI#\n",
      "Conventional Generative AI has already shown to be a revolutionary change for companies, helping their employees reach new levels of productivity. LLMs like ChatGPT are assisting users with a rapidly growing list of applications that include writing business plans, writing and improving code, writing marketing copy, and even providing healthier recipes for a specific type of diet. Ultimately, much of what users are doing is getting done faster.\n"
     ]
    }
   ],
   "source": [
    "# USING DOC2VEC SAVED MODEL\n",
    "\n",
    "# Load the saved model\n",
    "loaded_doc2vec_model = Doc2Vec.load(\"doc2vec_model.bin\")\n",
    "\n",
    "# Calculate the document vectors\n",
    "doc2vec_document_vectors = [loaded_doc2vec_model.dv[str(i)] for i in range(len(doc2vec_documents))]\n",
    "\n",
    "# User query for embedding\n",
    "doc2vec_user_query = [\"What are the advantages of RAG?\"]\n",
    "\n",
    "# Tokenize the new content\n",
    "doc2vec_tokenized_user_query = [content.lower().split() for content in doc2vec_user_query]\n",
    "\n",
    "# Infer the vector for the new content\n",
    "doc2vec_user_query_vector = loaded_doc2vec_model.infer_vector(doc2vec_tokenized_user_query[0])\n",
    "\n",
    "# Calculate cosine similarity between the new content vector and the document vectors\n",
    "doc2vec_similarity_scores = cosine_similarity([doc2vec_user_query_vector], doc2vec_document_vectors)\n",
    "\n",
    "# Find the index of the document with the highest similarity score\n",
    "doc2vec_top_doc_index = doc2vec_similarity_scores.argmax()\n",
    "\n",
    "# Print the text of the top document\n",
    "print(\"\\nDoc2Vec Top Document:\\n\", doc2vec_documents[doc2vec_top_doc_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ce29e-f71b-4efa-8d44-beda4b9bf11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING BERT\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Extract the text content from the splits\n",
    "bert_documents = [split.page_content for split in splits]\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Get the vector size of the BERT embeddings\n",
    "bert_vector_size = bert_model.config.hidden_size\n",
    "print(f\"Vector size of BERT (base-uncased) embeddings: {bert_vector_size}\\n\")\n",
    "\n",
    "# Tokenize the documents\n",
    "bert_tokenized_documents = [bert_tokenizer(doc, return_tensors='pt', max_length=512, truncation=True) for doc in bert_documents]\n",
    "\n",
    "# Calculate the document embeddings\n",
    "bert_document_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for doc in bert_tokenized_documents:\n",
    "        bert_outputs = bert_model(**doc)\n",
    "        bert_doc_embedding = bert_outputs.last_hidden_state[0, 0, :].numpy()\n",
    "        bert_document_embeddings.append(bert_doc_embedding)\n",
    "\n",
    "# New content (question) for embedding\n",
    "bert_user_query = [\"What are the advantages of RAG?\"]\n",
    "\n",
    "# Tokenize the new content\n",
    "bert_tokenized_user_query = bert_tokenizer(bert_user_query[0], return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "# Calculate the embedding for the new content\n",
    "bert_user_query_embedding = []\n",
    "with torch.no_grad():\n",
    "    bert_outputs = bert_model(**bert_tokenized_user_query)\n",
    "    bert_user_query_embedding = bert_outputs.last_hidden_state[0, 0, :].numpy()\n",
    "\n",
    "# Calculate cosine similarity between the new content embedding and the document embeddings\n",
    "bert_similarity_scores = cosine_similarity([bert_user_query_embedding], bert_document_embeddings)\n",
    "\n",
    "# Find the index of the document with the highest similarity score\n",
    "bert_top_doc_index = bert_similarity_scores.argmax()\n",
    "\n",
    "# Print the text of the top document\n",
    "print(\"BERT Top Document:\\n\", bert_documents[bert_top_doc_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b13568c-d633-464d-8c43-0d55f34cc8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e5aef-4c7f-4302-a2e6-ce1abe70e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the first result using the new content\n",
    "result = retriever.get_relevant_documents(\"What are the advantages of RAG?\")[0]\n",
    "\n",
    "# Print the retrieved document\n",
    "print(\"\\nRetrieved Document:\\n\", result.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce8df01-925b-45b5-8fb8-17b5c40c581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RETRIEVAL and GENERATION ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac053d8-b871-4b50-b04e-28dec9fb3b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = hub.pull(\"jclemens24/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef30632-13dd-4a34-af33-cb8fab94f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevance check prompt\n",
    "relevance_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Given the following question and retrieved context, determine if the context is relevant to the question.\n",
    "    Provide a score from 1 to 5, where 1 is not at all relevant and 5 is highly relevant.\n",
    "    Return ONLY the numeric score, without any additional text or explanation.\n",
    "\n",
    "    Question: {question}\n",
    "    Retrieved Context: {retrieved_context}\n",
    "\n",
    "    Relevance Score:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8975479-b3e3-481d-ad7b-08b4eb3faaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb6d70c-42ef-4bda-9607-48f02c941280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9db713-f705-4b65-800e-2c4e3d0e4ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(llm_output):\n",
    "    try:\n",
    "        score = float(llm_output.strip())\n",
    "        return score\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "# Chain it all together with LangChain\n",
    "def conditional_answer(x):\n",
    "    relevance_score = extract_score(x['relevance_score'])\n",
    "    if relevance_score < 4:\n",
    "        return \"I don't know.\"\n",
    "    else:\n",
    "        return x['answer']\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | RunnableParallel(\n",
    "        {\"relevance_score\": (\n",
    "            RunnablePassthrough()\n",
    "            | (lambda x: relevance_prompt_template.format(question=x['question'], retrieved_context=x['context']))\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        ), \"answer\": (\n",
    "            RunnablePassthrough()\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )}\n",
    "    )\n",
    "    | RunnablePassthrough().assign(final_answer=conditional_answer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c2ab0-9191-40f7-abf2-681f1c751429",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30177a-f9ab-45e4-812d-33b0f97325bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Question - relevant question\n",
    "result = rag_chain_with_source.invoke(\"What are the advantages of using RAG?\")\n",
    "relevance_score = result['answer']['relevance_score']\n",
    "final_answer = result['answer']['final_answer']\n",
    "\n",
    "print(f\"Relevance Score: {relevance_score}\")\n",
    "print(f\"Final Answer:\\n{final_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3da0d9-4b61-434b-afae-24678cd25d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Interface\n",
    "def process_question(question):\n",
    "    result = rag_chain_with_source.invoke(question)\n",
    "    relevance_score = result['answer']['relevance_score']\n",
    "    final_answer = result['answer']['final_answer']\n",
    "    sources = [doc.metadata['source'] for doc in result['context']]\n",
    "    source_list = \", \".join(sources)\n",
    "    return relevance_score, final_answer, source_list\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=process_question,\n",
    "    inputs=gr.Textbox(label=\"Enter your question\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Relevance Score\"),\n",
    "        gr.Textbox(label=\"Final Answer\"),\n",
    "        gr.Textbox(label=\"Sources\")\n",
    "    ],\n",
    "    title=\"LLM Question Answering\",\n",
    "    description=\"Enter a question and get the relevance score, final answer, and sources from the LLM.\"\n",
    ")\n",
    "\n",
    "demo.launch(share=True, debug=True, auth=(\"admin\", \"pass1234\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d3d0e-cb69-4cc5-bbef-db2ee947321b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55997b3e-100d-44a0-b378-daa4c4f3cfd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35328b-ffdb-4c19-ac0b-48f05440bc71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
